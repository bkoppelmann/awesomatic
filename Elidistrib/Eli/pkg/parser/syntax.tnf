\input texinfo  @c -*-texinfo-*-
@comment Copyright 1992, The Regents of the University of Colorado
@comment %**start of header
@setfilename syntax
@settitle Syntactic Analysis
@setchapternewpage odd
@comment %**end of header

@titlepage
@center @titlefont{Syntactic Analysis}
@sp 1
@center $Revision: 1.26 $
@include org.tnf
@page
@vskip 0pt plus 1filll
Copyright @copyright{} 1992, 1994, 1995, 1996, 1998, 2004, 2008, 2009, 2012
The Regents of the University of Colorado

@quotation
Permission is granted to make and distribute verbatim
copies of this manual provided the copyright notice and
this permission notice are preserved on all copies.
@end quotation
@end titlepage

@node Top
@top Syntactic Analysis

The purpose of syntactic analysis is to determine the structure of the
input text.
This structure consists of a hierarchy of @dfn{phrases},
@findex phrase
the smallest of which are the @dfn{basic symbols}
@findex basic symbol
and the largest of which is the @dfn{sentence}.
@findex sentence
@findex node
The structure can be described by a tree with one node for each phrase.
Basic symbols are represented by values stored at the nodes.
The root of the tree represents the sentence.
@findex root

This manual explains how to use a @file{.con} specification to describe
the set of all possible phrases that could appear in sentences of a language.
It also discusses methods of resolving ambiguity in such descriptions, and
how to carry out arbitrary actions during the recognition process itself.
The use of @file{.perr} specifications to improve the error
recovery of the generated parser is described as well.

Computations over the input can be written with attribute grammar
specifications that are based on an abstract syntax.  The abstract syntax
describes the structure of an abstract syntax tree, much the way the concrete
syntax describes the phrase structure of the input.  Eli uses a tool,
called @dfn{Maptool}, that automatically generates the abstract syntax
@findex Maptool
tree based on an analysis of the concrete and abstract syntaxes and
user specifications given in files of type @file{.map}.  This manual
will describe the rules used by Maptool to determine a unique correspondence
between the concrete and abstract syntax and the information users can
provide in @file{.map} files to assist in the process.

This manual will also discuss how Maptool makes it possible to only
partially specify the concrete and abstract syntaxes, as long as together they
specify a complete syntax.

Although Maptool simplifies the task of relating the phrase structure of a
language to the abstract syntax on which a computation is based, it is
cometimes necessary to use a parser that was not generated by Eli to
analyze phrase structure.
In that case, the relationship between phrase structure and abstract syntax
must be embedded in a hand-coded tree construction module.
The last section of this manual explains how such a module is implemented,
and describes the way in which Eli supports that implementation and manages
its integration with the generated tree computations.

@menu
* Phrases::	        Context-Free Grammars and Parsing
* Mapping::	        The Relationship Between Phrases and Tree Nodes
* Conflicts::	        How to Resolve Parsing Conflicts
* Actions::	        Carrying Out Actions During Parsing
* Error Recovery::      Improving Error Recovery in the Generated Parser
* Foreign::             Using Foreign parser generators
* Grammars::	        Grammars for the Specification Files
* Index::
@end menu

@node Phrases
@chapter Context-Free Grammars and Parsing

A @dfn{context-free grammar}
@findex grammar
@findex context-free grammar
is a formal system that describes a language by
specifying how any legal text can be derived from a distinguished symbol
called the @dfn{axiom},
@findex axiom
or @dfn{sentence symbol}.
@findex sentence symbol
It consists of a set of @dfn{productions},
@findex production
each of which states that a given symbol can be replaced by a given sequence
@findex sequence
of symbols.
To derive a legal text,
@findex derivation
the grammar is used as data for the following algorithm:

@enumerate
@item
Let @code{text} be a single occurrence of the axiom.

@item
If no production states that a symbol currently in @code{text} can be replaced
by some sequence of symbols, then stop.

@item
Rewrite @code{text} by replacing one of its symbols with a sequence
according to some production.

@item
Go to step (2).
@end enumerate

@noindent
When this algorithm terminates, @code{text} is a legal text in the language.
The @dfn{phrase structure}
@findex phrase structure
of that text is the hierarchy of sequences used in its derivation.

Given a context-free grammar that satisfies certain conditions,
Eli can generate a @dfn{parsing routine}
@findex parsing routine
to determine the derivation (and hence the phrase structure) of any legal text.
This routine will also automatically detect and report any errors
@findex syntactic error reports during parsing
@findex parser error reports
@findex error reports during parsing
@findex reports of syntactic errors
in the text, and repair
@findex repairing syntactic errors
them to produce a correct phrase structure
(which may not be that intended by the person who wrote the erroneous text).

@menu
* Notation::	How to describe a context-free grammar
* Meaning::	Using structure to convey meaning
@end menu

@node Notation
@section How to describe a context-free grammar

Each production of a context-free grammar consists of a symbol to be replaced
and the sequence that replaces it.
This can be represented in a type-@file{con} file
@findex @file{.con} file, purpose
@findex @file{con} file, purpose
@findex type-@file{con} file, purpose
by giving the symbol to be replaced, followed by a colon,
followed by the sequence that replaces it, followed by a period:

@example
Assignment: Variable ':=' Expression.
StatementList: .
Statement:
   'if' Expression 'then' Statement
   'else' Statement.
@end example

@noindent
The first production asserts that the symbol @code{Assignment} can be replaced
by the sequence consisting of the three symbols @code{Variable}, @code{':='},
and @code{Expression}.
Any occurrence of the symbol @code{StatementList} can be replaced by an empty
sequence according to the second production.
In the third production, you see that new lines can be used as separators
in the description of a production.  This notation is often more commonly
referred to as @dfn{Backus Naur Form}, or just @dfn{BNF}.
@findex Backus Naur Form
@findex BNF

@menu
* Nonterminals::	Representations of symbols to be replaced
* Terminals::		Representations of character strings
* EBNF::                Using extended BNF to describe more complex rules
@end menu

@ifinfo
@node Nonterminals
@subsection Representations of symbols to be replaced
@end ifinfo

Symbols that are to be replaced are called @dfn{nonterminals},
@findex nonterminal symbols in a grammar
and are always represented by @dfn{identifiers}.
@findex identifier, form of
(An identifier is a sequence of letters and digits, the first of which is a
letter.)
Every nonterminal must appear before a colon in at least one production. 
The axiom is a nonterminal that appears before the colon in exactly one
production, and does not appear between the colon and the period in any
production.
There must be exactly one nonterminal satisfying the conditions for the axiom.

@ifinfo
@node Terminals
@subsection Representations of character strings
@end ifinfo

Symbols that cannot be replaced are called @dfn{terminals},
@findex terminal symbols in a grammar
and may be represented by either identifiers or @dfn{literals}.
@findex literal, form of
(A literal is a sequence of characters bounded by apostrophes (@kbd{'}).
An apostrophe appearing within a literal is represented by two successive
apostrophes.)
No terminal may appear before a colon in any production.
Terminals represent character strings that are recognized by the lexical
analyzer (@pxref{Specifications,,, lex, Lexical Analysis}).
@findex lexical analyzer

@ifinfo
@node EBNF
@subsection Using extended BNF to describe more complex rules
@end ifinfo

@dfn{Extended BNF} allows the use of certain operators on the right hand side
of a production.  These operators are designed to be short-hands to simplify
the grammar description.  Rules with extended BNF operators can be
translated into rules which use only the strict BNF constructs described
so far.  While the use of extended BNF constructs is supported for the
concrete syntax description in Eli, only strict BNF constructs are allowed
in the abstract syntax.  When it comes time to deduce the correspondence
between the concrete and abstract syntax, Maptool operates on the abstract
syntax and a version of the concrete syntax in which all rules containing 
extended BNF constructs have been translated into equivalent strict
BNF rules.

The remainder of this section is devoted to describing how each of the extended
BNF constructs are translated to their strict BNF equivalents.  Note that
most of the EBNF constructs require the introduction of generated symbols
for their strict BNF translation.  Users are strongly discouraged from using
these constructs in instances where attribution is required for those
contexts, because changes in the grammar will change the names of the
generated symbols used.

The most appropriate use of EBNF constructs that introduce generated
symbols is when matching the LIDO
@code{LISTOF} construct, since the @code{LISTOF} construct makes no
assumptions about the phrase structure of the list.
For a description of the @code{LISTOF} construct, see
@ref{Productions,,,lidoref, LIDO - Reference Manual}.

@menu
* Slash::	        Collecting replacements for the same symbol
* Square Brackets::	Optional symbols
* Asterisk::	        Zero or more occurrences
* Plus::	        One or more occurrences
* Double Slash::	Separator construct
* Parentheses::	        Symbol grouping
@end menu

@ifinfo
@node Slash
@subsubsection Collecting replacements for the same symbol
@end ifinfo

When a grammar contains many productions specifying replacement of the same
nonterminal, a slash, denoting @dfn{alternation}
@findex alternation
can be used to avoid re-writing the symbol being replaced:

@example
Statement:
   Variable ':=' Expression /
   'if' Expression 'then' Statement 'else' Statement /
   'while' Expression 'do' Statement .
@end example

This alternation specifies three productions.
The nonterminal to be replaced is @code{Statement} in each case.
Possible replacement sequences are separated by slashes (@kbd{/}).
The strict BNF translation for the above example is:

@example 
Statement: Variable ':=' Expression .
Statement: 'if' Expression 'then' Statement 'else' Statement . 
Statement: 'while' Expression 'do' Statement . 
@end example

Alternation does not introduce any generated symbols and has a very
straight-forward translation.  As a result, it is the most heavily used
of the EBNF constructs.

@ifinfo
@node Square Brackets
@subsubsection Describing optional symbols within a rule
@end ifinfo

Square brackets are used to denote that the set of symbols
enclosed by the brackets are optional.  In the following
example, @code{Constants} and @code{Variables} are optional,
but @code{Body} is not:

@example
Program: [Constants] [Variables] Body .
@end example

The strict BNF translation of this construct is to generate
a rule for each possible permutation of the right hand side.
In the case of the above example, the following four rules
would result:

@example
Program: Body .
Program: Variables Body .
Program: Constants Body .
Program: Constants Variables Body .
@end example

While the translation doesn't introduce any generated symbols,
indiscriminate use of this construct may lead to less readable specifications.

@ifinfo
@node Asterisk
@subsubsection Specifying zero or more occurrences
@end ifinfo

An asterisk (or star) is used to denote zero or more occurrences
of the phrase to which it is applied.  In the following example,
@code{Program} consists of zero or more occurrences of @code{Variable}
followed by @code{Body}:

@example
Program: Variable* Body .
@end example

The strict BNF translation of this construct requires the introduction
of a generated symbol.  Generated symbols begin with the letter @code{G}
and are followed by a unique number.  Generated symbols are chosen to not
conflict with existing symbols in the concrete syntax.  No check is
performed to ensure that the generated symbols do not conflict with
symbols in the abstract syntax, so users should avoid using symbols
of this form in their abstract syntax.  The translation
for the above example is as follows:

@example
Program: G1 Body .
G1: G1 Variable .
G1: .
@end example

@ifinfo
@node Plus
@subsubsection Specifying one or more occurrences
@end ifinfo

A plus is used to denote one or more occurrences
of the phrase to which it is applied.  In the following example,
@code{Program} consists of one or more occurrences of @code{Variable}
followed by @code{Body}:

@example
Program: Variable+ Body .
@end example

The strict BNF translation of this construct is similar to the translation
of the asterisk (@pxref{Asterisk}).  The translation
for the above example is as follows:

@example
Program: G1 Body .
G1: G1 Variable .
G1: Variable .
@end example

@ifinfo
@node Double Slash
@subsubsection EBNF Separator construct
@end ifinfo

A double slash is used to denote one or more occurrences of a phrase
separated by a symbol.  In the following example, @code{Input} is a
sequence of one or more @code{Declaration}'s separated by a comma:

@example
Input: Declaration // ',' .
@end example

The strict BNF translation for the above example is as follows:

@example
Input: G1 .
G1: G2 .
G1: G1 ',' G2 .
G2: Declaration .
@end example

Note that all of the EBNF constructs, except the single slash (for alternation)
have higher precedence than the separator construct.

@ifinfo
@node Parentheses
@subsubsection Grouping constructs in EBNF expressions
@end ifinfo

Parentheses are used to group EBNF constructs.  This is used primarily
to apply other EBNF operators to more than a single symbol.  For example:

@example
Program: (Definition Use)+ .
@end example

In this example, we want to apply the Plus operator to the concatenation of
a @code{Definition} and a @code{Use}.  The result denotes one or more
occurrences of @code{Definition}'s followed by @code{Use}'s.  The strict
BNF translation for the above is:

@example
Program: G2 .
G1: Definition Use .
G2: G1 .
G2: G2 G1 .
@end example

This is identical to the translation for the Plus operator operating on a
single symbol, except that another generated symbol is created to represent
the parenthetical phrase.

Note that a common error is to introduce parentheses where they are not
needed.  This will result in the introduction of unexpected generated
symbols.

@node Meaning
@section Using structure to convey meaning

A production is a construct with two components: the symbol to be replaced
and the sequence that replaces it.
We defined the meaning of the production in terms of those components,
saying that whenever the symbol was found in @code{text}, it could be
replaced by the sequence.
This is the general approach that we use in defining the meaning of constructs
@findex meaning of a construct
in any language.
For example, we say that an assignment is a statement with two components,
a variable and an expression.
The meaning of the assignment is to replace the value of the variable with
the value resulting from evaluating the expression.

The context-free grammar for a language specifies a ``component'' relationship.
Each production says that the components of the phrase represented by the
symbol to be replaced are the elements of the sequence that replaces it.
To be useful, the context-free grammar for a language should embody exactly the
relationship that we use in defining the meanings of the constructs of that
language.

@menu
* Precedence::	Operator precedence
* Association::	Operator associativity
* Scope::	Scope rules for declarations
@end menu

@node Precedence
@subsection Operator precedence

Consider the following expressions:

@example
A + B * C
(A + B) * C
@end example

@noindent
In the first expression, the operands of the addition are the variable
@code{A} and the product of the variables @code{B} and @code{C}.
The reason is that in normal mathematical notation, multiplication takes
precedence over addition.
@findex precedence rules
@findex rules, precedence
@findex operator precedence
Parentheses have been used in the second expression to indicate that the
operands of the multiplication are the sum of variables @code{A} and
@code{B}, and the variable @code{C}.

The general method for embodying this concept of operator precedence in a
context-free grammar for expressions is to associate a distinct nonterminal
with each precedence level, and one with operands that do not contain
``visible'' operators.
For our expressions, this requires three nonterminals:

@table @code
@item Sum
An expression whose operator is @code{+}

@item Term
An expression whose operator is @code{*}

@item Primary
An expression not containing ``visible'' operators
@end table

The productions that embody the concept of operator precedence would then
be:

@example
Sum: Sum '+' Term / Term.
Term: Term '*' Primary / Primary.
Primary: '(' Sum ')' / Identifier.
@end example

@node Association
@subsection Operator associativity

Consider the following expressions:

@example
A - B - C
A ** B ** C
A < B < C
@end example

@noindent
Which operator has variable @code{B} as an operand in each case?

This question can be answered by stating an @dfn{association}
@findex association rules
@findex rules, association
@findex operator association
for each operator:
If @code{-} is ``left-associative'',
@findex left-associative
then the first expression is interpreted as though it had been written
@code{(A-B)-C}.
Saying that @code{**} is ``right-associative''
@findex right-associative
means that the second expression is interpreted as though it had been written
@code{A**(B**C)}.
The language designer may wish to disallow the third expression by saying
that @code{<} is ``non-associative''.
@findex non-associative

Association rules are embodied in a context-free grammar by selecting
appropriate nonterminals to describe the operands of an operator.
For each operator, two nonterminals must be known:
the nonterminal describing expressions that may contain that operator, and
the nonterminal describing expressions that do not contain that operator
but may be operands of that operator.
Usually these nonterminals have been established to describe operator
precedence.
Here is a typical set of nonterminals used to describe expressions:

@table @code
@item Relation
An expression whose operator is @code{<} or @code{>}

@item Sum
An expression whose operator is @code{+} or @code{-}

@item Term
An expression whose operator is @code{*} or @code{/}

@item Factor
An expression whose operator is @code{**}

@item Primary
An expression not containing ``visible'' operators
@end table

The association rules discussed above would therefore be expressed by the
following productions
(these are @emph{not} the only productions in the grammar):

@example
Sum: Sum '-' Term.
Factor: Primary '**' Factor.
Relation: Sum '<' Sum.
@end example

@noindent
The first production says that the left operand of @code{-} can contain
other @code{-} operators, while the right operand cannot (unless the
subexpression containing them is surrounded by parentheses).
Similarly, the right operand of @code{**} can contain other @code{**}
operators but the left operand cannot.
The third rule says that neither operand  of @code{<} can contain other
@code{<} operators.

@node Scope
@subsection Scope rules for declarations

Identifiers
@findex identifier declarations, scope issues
@findex declarations, scope issues
are normally given meaning by declarations.
The meaning given to an identifier by a particular declaration holds over
some portion of the program, called the @dfn{scope}
@findex scope
of that declaration.
A context-free grammar for a language should define a phrase structure that
is consistent with the scope rules of that language.

For example, the declaration of a procedure @code{P} within the
body of procedure @code{Q} gives meaning to the identifier @code{P}, and
its scope might be the body of the procedure @code{Q}.
If @code{P} has parameters, the scope of their declarations (which are
components of the procedure declaration) is the body of procedure @code{P}.

Now consider the following productions describing a procedure declaration:
@findex procedure declaration, scope issues

@example
procedure_declaration: 'procedure' procedure_heading procedure_body.
procedure_heading:
   ProcIdDef formal_parameter_part ';' specification_part.
@end example

@noindent
Notice that the phrase structure induced by these productions is
inconsistent with the postulated scope rules.
The declaration of @code{P} (@code{ProcIdDef}) is in the same phrase
(@code{procedure_heading}) as the declarations of the formal parameters.
This defect can be remedied by a slight change in the productions:

@example
procedure_declaration: 'procedure' ProcIdDef ProcRange.
ProcRange:
   formal_parameter_part ';' specification_part procedure_body.
@end example

@noindent
Here the formal parameters and the body have both been made components of a
single phrase (@code{ProcRange}), which defines the scope of the formal
parameter declarations.
The declaration of @code{P} lies outside of this phrase, thus allowing its
scope to be differentiated from that of the formal parameters.

@node Mapping
@chapter The Relationship Between Phrases and Tree Nodes

@code{RULE} declarations in files of type @file{lido} describe the
structure of the abstract syntax tree over which computations are
performed.  Eli will create a routine to construct an abstract syntax
tree if any tree computations
@findex tree computations
are specified
(@pxref{Tree,Tree Structure,Tree Structure,comptrees,
LIDO -- Computations in Trees}).
In order to do this, Eli must be able to deduce a unique correspondence
between the concrete and abstract syntaxes, such that for each rule
in the concrete syntax it is possible to uniquely determine what
abstract syntax tree fragment to build.  The tool within Eli that
does this is called Maptool.  In addition to generating a routine to
construct the abstract syntax tree, Maptool will also deduce complete
versions of the concrete and abstract syntaxes if only incomplete versions
of each are provided by the user.  This can only be done if the two
syntaxes can together form a complete syntax.

The concrete syntax is provided by the user in files of type @file{con}.
Since EBNF constructs are allowed in these files, they are first translated
into their strict BNF equivalents before being processed by Maptool
(@pxref{EBNF}).  The abstract syntax is extracted from the @code{RULE}
declarations made in files of type @file{lido}
(@pxref{Rule Specifications,,,lidoref, LIDO - Reference Manual}).

The remainder of this section will discuss how Maptool deduces the
correspondence between the two syntaxes, the use of files of type @file{map}
to influence the mapping process, and some usage hints.

@menu
* Process::     Mapping syntaxes and generating complete versions
* Map Files::   Specifications to influence the mapping process
* BOTTOMUP::    Influences of BOTTOMUP specifications on mapping
* Hints::       Hints for syntax development
@end menu

@node Process
@section Syntax mapping process

Maptool begins by matching any @code{LISTOF} constructs that appear
in the abstract syntax to any appropriate concrete rules.  The next
phase examines each concrete rule not matched in the previous phase
and tries to find a matching abstract syntax rule.  After all matching
is complete, unmatched concrete rules are added to the abstract syntax
and unmatched abstract rules are added to the concrete syntax.  There
are a few exceptions to this as are noted in the remainder of this
section.

While the most obvious benefit to having Maptool deduce syntax fragments
from one syntax and place them in the other is to reduce the amount
of typing required, the more important advantage is the support it
gives for incremental development.  It allows the user to only specify
those portions of the syntax with which they are concerned at the moment.

@menu
* Chain Rules::         Definitions for different kinds of chain rules
* LISTOF::              Matching abstract @code{LISTOF} constructs
* Rule Matching::       Matching the remaining rules
* Completion::          Generation of the complete syntaxes
@end menu

@node Chain Rules
@subsection Chain rule definitions

Chain rules have different behavior than other rules during the matching
process. Descriptions for three different kinds of chain rules are given here
to assist in the explanations given in the remainder of this section:

@table @dfn

@item Chain Rule
@findex chain rules
A normal chain rule is a rule in which there is exactly one symbol on
the right hand side of the rule that is not equivalent to the left
hand side.  For example, @samp{X ::= Y} where X is not equivalent to Y
is a chain rule.

@item Trivial Chain Rule
@findex trivial chain rules
A trivial chain rule is a chain rule in which the left hand side is
equivalent to the right hand side.  This typically happens when a symbolic
equivalence class is defined that includes both the left hand side symbol
and the right hand side symbol (@pxref{Symbol Mapping}).

@item Literal Chain Rule
@findex literal chain rules
A literal chain rule is similar to a trivial chain rule, except that it
also has literal symbols on its right hand side.  A typical example of
this is the rule @samp{Expr ::= '(' Expr ')'}.

@end table

Based on the above definition for normal chain rules, we define @dfn{coercions}
@findex coercions
between symbols.  A symbol @code{X} can be coerced to a symbol @code{Y} if
there is a chain rule with @code{X} on the right hand side and @code{Y} on
the left hand side.  Coercions are also transitive.  If @code{X} is coercible
to @code{Y} and @code{Y} is coercible to @code{Z}, then @code{X} is also
coercible to @code{Z}.  A symbol is also considered coercible to itself.

@node LISTOF
@subsection Matching the @code{LISTOF} construct

The @code{LISTOF} construct denotes zero or more occurrences of the
elements that appear on its right hand side.  It does not dictate
the ordering of those right hand side symbols or any delimiters that
may be used to separate them.  The ordering and delimiters are determined
by concrete rules.  In simple terms, Maptool begins with the left hand
side of the @code{LISTOF} and recursively matches rules until it finds
the right hand side elements.  The next paragraph gives a more precise
description.

An abstract @code{LISTOF} construct is matched by starting with the
symbol on the left hand side of the LISTOF.  All concrete rules with
equivalent left hand side symbols are added to the set of matched rules.
For each rule added to the set, the right hand side symbols are examined.
Of these symbols, literal symbols are ignored.  If terminal symbols are
encountered that aren't coercible to the symbols appearing on the right
hand side of the @code{LISTOF}, an error is signalled, because the left
hand side of the @code{LISTOF} may not derive symbols other than those that
appear on the right hand side.  For each nonterminal symbol that isn't
coercible to one of the right hand side symbols, the concrete rules that
have that symbol on their left hand side are added to the set.  The process
continues until no more rules can be added to the set.

The intermediate nonterminal symbols that are encountered as new concrete
rules are added to the set may not appear on the right hand side of other
concrete rules.

If Maptool doesn't find any concrete rules to match a @code{LISTOF}, it
will generate a canonical left recursive representation.  For the
list:

@example
RULE: Program LISTOF Declaration | Statement END;
@end example

Maptool would generate the following:

@example
Program: LST_Program .
LST_Program: LST_Program Declaration .
LST_Program: LST_Program Statement .
LST_Program: .
@end example

This specifies zero or more occurrences of @code{Declaration}'s and
@code{Statement}'s.

There is one other important thing to note about the @code{LISTOF} construct.
Attribute computations associated with a @code{LISTOF} construct can
just as easily be written as symbol computations on the symbols of the
@code{LISTOF}.  The advantage to using the @code{LISTOF} construct is
that it becomes possible to generate an abstract syntax tree structure
which allows for more efficient traversal.  In order to construct this
special tree structure, it is sometimes necessary to insert an additional
chain rule into the concrete syntax at the root of the @code{LISTOF}.

This is the case when the rules matching the @code{LISTOF} have a recursive
occurrence of the left hand side symbol.  As an example, the @code{LISTOF}
construct shown above might be written as follows in the concrete syntax:

@example
Program: Program Declaration .
Program: Program Statement .
Program: .
@end example

As you can see, the root of the @code{LISTOF}, @code{Program} is used both
on the left hand side and right hand side of rules that match the @code{LISTOF}
construct, meaning that it is used recursively.  If the @code{LISTOF}
construct is provided in a @file{.lido} file, Maptool must introduce the
chain rule @samp{Program ::= LST_Program} and change other occurrences of
@code{Program} to @code{LST_Program} in order to build the efficient tree
structure.

Users should be aware that it is possible for the addition of this chain
rule to cause LALR(1) conflicts for the parsability of the concrete syntax
that do not appear in the absence of the @code{LISTOF} construct.
In these cases, users must either rewrite the concrete syntax or
avoid the use of the @code{LISTOF} construct to avoid the problem.

@node Rule Matching
@subsection Matching remaining rules

After all @code{LISTOF} constructs have been matched, Maptool attempts
to match the remaining concrete rules to rules given in the abstract
syntax.  A match is determined if the signature of the concrete rule
is equivalent to the signature of an abstract rule or coercions
(@pxref{Chain Rules}) exist between any symbols which differ in the
signatures.  Remember that symbolic equivalence classes are applied
to concrete rules before this matching takes place, so symbols in the
signatures are considered equivalent if they belong to the same equivalence
class.

For example, consider the following abstract rules:

@example
RULE: Declaration ::= IdDef Type END;
RULE: IdDef ::= Identifier END;
@end example

The following concrete rule will match the first of the above abstract
rules, because of the coercion defined between @code{Identifier} and
@code{IdDef}:

@example
Declaration: Identifier Type .
@end example

The reason for doing this is to distinguish semantically between
@findex semantic disambiguation
occurrences of @code{Identifier}'s in different contexts.  In the above
example, we have used @code{IdDef} to represent a definition of an
@code{Identifier}.  In another place in the grammar, we may want to
refer to uses of identifiers instead and use the symbol @code{IdUse}.
Note that use of chain rules in the manner just described makes it
impossible to perform attribute computations during tree construction
(@pxref{Constraints}).

It is possible for Maptool to detect multiple possible matching abstract
rules for a single concrete rule.  Maptool signals an error in this case
that must be fixed by changing the grammar to disambiguate the contexts.

@node Completion
@subsection Complete generated concrete and abstract syntaxes

After rule matching is complete, unmatched concrete rules, except
trivial chain rules and literal chain rules (@pxref{Chain Rules})
are added to the abstract syntax.  The reason for this is that
trivial chain rules are meaningless in the abstract syntax and literal
chain rules are only meaningful if they have attribute computations
associated with them, in which case they would already have been
specified as part of the abstract syntax.  

@findex MAPCHAINS keyword
@findex preserving literal chain rules
@findex literal chain rules, preserving
@findex literal chain rules and Idem
@findex Idem and literal chain rules
Sometimes it is desirable to include literal chain rules in the abstract
syntax even when the user has not explicitly included them there.  A
typical situation where this occurs is when generating output conforming
to the concrete syntax using the Idem tool
(@pxref{idem,Textual unparser,Textual unparser,idem,
Abstract Syntax Tree Unparsing}).
In this situation the output must contain all
literals hence the literal chain rules must be in the abstract syntax so
that Idem can generate output patterns for them.  To preserve the
literal chain rules in the abstract syntax use the @code{MAPCHAINS}
keyword in a specification (@pxref{Mapping Chain Rules}).

Unmatched abstract rules are included in the concrete syntax except in
the following instances:

@itemize @bullet

@item
The rule is a chain rule whose left hand side is not a symbol in
the concrete syntax.  Adding the rule to the concrete syntax in this
case would cause the concrete syntax to be disconnected.

@item
The rule can only be part of a computed subtree
(@pxref{Computed Subtrees,,,lidoref, LIDO - Reference Manual}).
This is true if the rule is only reachable from the root symbol
if symbols preceded by a @kbd{$} are included.

@end itemize

Users can use the @code{:consyntax} product
(@pxref{consyntax,,,pp,Products and Parameters}) to view the
complete version of the concrete syntax.

The @code{:abstree} product
(@pxref{abstree,,,pp, Products and Parameters}) is used to view
the complete abstract tree grammar.  The @code{:absyntax} product
(@pxref{absyntax,,,pp, Products and Parameters}) by contrast only
shows the abstract syntax rules which are not part of computed subtrees.

@node Map Files
@section User mapping specifications

Files of type @file{map} can be provided by the user to influence the
way in which certain rules are matched.  The syntax of map files
can be found with other grammar description towards the end of this
document (@pxref{Grammars}).

There are currently three ways in which the mapping can be affected.
The first are symbolic equivalence classes, which group together
symbols that have the same semantic meaning.  The second method is to
map specific rules.  Using this method, concrete rules can be rewritten
and/or reordered to match a specific abstract rule.  The third method
controls the elimination of literal chain rules.


@menu
* Symbol Mapping::        Specifying symbolic equivalence classes
* Rule Mapping::          Rewriting a concrete rule
* Mapping Chain Rules::   Preserving literal chain rules
@end menu

@node Symbol Mapping
@subsection Specifying symbolic equivalence classes

@findex Symbol Mapping
@findex MAPSYM keyword

Symbolic equivalence classes are used to group together symbols
appearing in the concrete syntax because the semantics of the symbols
are equivalent.  As a result, a single symbol can be used to represent
all of the members of the symbolic equivalence class in the abstract
syntax.  This representative symbol can either be one of the concrete
symbols or a new symbol altogether.  Symbolic equivalence classes are
specified in files of type @file{map}.  A series of symbolic
equivalences must be preceded by the keyword @code{MAPSYM}.  An
equivalence class is then specified by giving the representative symbol
(the symbol to appear in the abstract syntax), followed by @kbd{::=} and
the list of symbolically equivalent symbols from the concrete syntax
terminated by a period.  For example, the following specification says
that a @code{Primary}, @code{Factor}, and @code{Expr} belong to the same
equivalence class:

@example
MAPSYM
Expr ::= Primary Factor .
@end example

Application of symbolic equivalence classes to rules in the concrete syntax
is done before the matching process begins.  Symbolic equivalence classes
can only be created for symbols which are either all nonterminals or
all terminals (@pxref{Notation}).  An error message will also be issued
if a symbolic equivalence class specification includes abstract syntax
symbols on the right hand side, since each abstract syntax symbol
represents its own equivalence class.

For backward compatibility with previous releases of Eli, symbolic
equivalence classes may also be specified in files of type @file{sym}.

@node Rule Mapping
@subsection Specifying rule mappings

@findex Rule Mapping
@findex MAPRULE keyword

Rule mapping allows users to rewrite a concrete rule for the purposes
of matching it to a specific abstract rule.  This is useful in cases
where two syntactically different constructs are semantically equivalent.
Consider the following expression language with bound identifiers:

@example
Computation: LetExpr / WhereExpr .
LetExpr: 'let' Definitions 'in' Expr .
WhereExpr: Expr 'where' Definitions .
@end example

In this example, @code{LetExpr} and @code{WhereExpr} are semantically
equivalent constructs, but the ordering of @code{Definitions} and @code{Expr}
are reversed and they use different literal symbols.
We'd like to only specify the semantic computations for the two constructs
once.  To do this, we can define a symbolic equivalence class for
@code{LetExpr} and @code{WhereExpr}:

@example
MAPSYM
BoundExpr ::= LetExpr WhereExpr .
@end example

The abstract rule that we can use to represent the two constructs is:

@example
RULE: BoundExpr ::= Definitions Expr END;
@end example

Finally, we must use rule mapping specifications to rewrite the two
concrete rules to match the abstract rule:

@example
MAPRULE
LetExpr: 'let' Definitions 'in' Expr < $1 $2 > .
WhereExpr: Expr 'where' Definitions < $2 $1 > .
@end example

The keyword @code{MAPRULE} precedes a group of rule mapping specifications
in the map file.  Each rule mapping begins with the concrete rule
to be rewritten followed by its rewritten form in angle brackets.  In
angle brackets, nonliteral symbols appear as positional parameters.
A positional parameter is specified with a @kbd{$} followed by a number
indicating which nonliteral symbol from the concrete rule is to be used.
Any literal symbols may also appear between the angle brackets.

An abstract syntax will sometimes have several rules with different names
but identical signatures.
For example, consider the case where dyadic expressions are represented by
abstract rules that do not contain operators:

@example
RULE Add: Expression ::= Expression Expression END;
RULE Mul: Expression ::= Expression Expression END;
...
@end example
@noindent
In this case, the rule mapping must specify the abstract rule name
explicitly in order to disambiguate the pattern match:

@example
MAPRULE
Expression: '(' Expression '+' Expression ')' < $1 $2 >: Add .
Expression: '(' Expression '*' Expression ')' < $1 $2 >: Mul .
...
@end example
@noindent
Rule names are optional, and may be omitted when the pattern match is
unambiguous (as in the bound variable example).

When rule matching proceeds, the concrete rule is seen in its rewritten
form.  An abstract syntax rule must exist in a LIDO
specification that corresponds to the rule mapping specification given.
Note that the use of rule mapping makes it impossible to perform
attribute computations during tree construction (@pxref{Constraints}).

@node Mapping Chain Rules
@subsection Preserving literal chain rules

@findex MAPCHAINS keyword
@findex literal chain rules, preserving
@findex preserving literal chain rules
The mapping process normally does not include literal chain rules in the
complete abstract syntax unless they appear in the user-supplied
abstract syntax (@pxref{Completion}).  Sometimes it is desirable to
preserve literal chain rules even if the user has not included them in
the abstract syntax.  To force literal chain rules to be included in the
abstract syntax, use the @code{MAPCHAINS} keyword.  The behavior is
unchanged if all literal chain rules already appear in the abstract syntax.

Care should be taken when using @code{MAPCHAINS} in conjunction with
attribution.  A specification using this keyword may require
more attribution than the same specification without it, because it
may be necessary to transfer attribute values from the child to the parent
or vice versa.  The presence of symbol computations for
the symbols occurring in the chain rules without the transfer computations
just mentioned may result in incorrect attribution without warning.

@node BOTTOMUP
@section Influences of BOTTOMUP specifications on mapping

The generation of the parsing grammar (the input to the parser) may be
influenced by @code{BOTTOMUP} specifications
(@pxref{Computations,,,lidoref,LIDO - Reference Manual})
specified in your attribute grammar.  This is because the parsing grammar
must ensure that the nodes of the abstract syntax tree are constructed
in a particular order in the presence of @code{BOTTOMUP} constraints.

In order to deal with this, Maptool must sometimes inject generated
chain rules into the parsing grammar to which tree building actions can
be attached.  These injected chain rules may cause the parsing grammar
to exhibit LALR(1) conflicts.  If so, an error will be reported to
indicate that the @code{BOTTOMUP} constraints you have provided cause
your grammar to not be parsable.

In trying to resolve such a conflict, it is useful to use the @code{:pgram}
derivation (@pxref{pgram,,, pp, Products and Parameters}) to be able to
view the parsing grammar that is submitted to the parser generator and
contains the injected chain rules.  It is also useful to use the
@code{:OrdInfo} derivation to get more information about how
@code{BOTTOMUP} constraints were introduced for specific rules.
Approaches to resolving such a problem include eliminating unnecessary
@code{BOTTOMUP} constraints from the attribute grammar or making changes
to the concrete syntax that allow the chain rules to be injected without
causing LALR(1) conflicts.

@node Hints
@section Syntax development hints

This section begins by describing typical patterns of syntax development.
This is followed by two more specific examples of how to use the mapping
techniques described in the previous sections.

@menu
* Patterns::    Typical patterns of syntax development
* Constraints:: Constraints on grammar mapping
* Abstraction::	Abstracting information from literals
* Expression::	Mapping expressions for overload resolution
@end menu

@node Patterns
@subsection Typical patterns of syntax development

When developing a translator for an existing language, the complete
concrete syntax is typically already available.  In these cases,
it is advantageous to start with the complete concrete syntax and
add symbolic equivalences and rule mapping specifications to suit
the attribute computations as they are being developed.

On the other hand, when designing a new language, it is easier to start
work by specifying attribute computations and adding concrete syntax rules
as necessary to resolve issues of precedence, associativity, and other
parsing ambiguities.

When errors relating to the syntax appear, it is strongly recommended that
the first course of action be to look at the complete generated versions of the
syntaxes by using the @code{:consyntax}, @code{:absyntax}, and
@code{:abstree} products (@pxref{Specifications,,,pp,Products and Parameters}).
Very often these problems are simply a result
of not correctly anticipating the matching process.

@node Constraints
@subsection Constraints on grammar mapping

The LIGA attribute grammar system allows users to specify
that the first pass of computations are to be performed as the
abstract syntax tree is being built.  This is specified either
by an option given in a LIGA control specification
@pxref{Order Options,,,lcl,LIGA Control Language} or by using
an additional keyword in an attribute grammar computation
@pxref{Computations,,,lidoref,LIDO - Reference Manual}.

Combining computations with tree construction, however, requires
that the tree be constructed in strict left-to-right and
bottom-to-top order.  In the presence of more advanced grammar
mappings, it is not possible to maintain this strict ordering.
For this reason, Maptool generates the LIGA control directive:

@example
ORDER: TREE COMPLETE ;
@end example

when it detects that one of these grammar mappings is required.
The control directive indicates that the tree
should be constructed completely before any computations
take place.

The grammar mappings which cause Maptool to emit these directives
are the use of chain rules in the abstract syntax that do
not exist in the concrete syntax (@pxref{Rule Matching})
and any use of rule mapping (@pxref{Rule Mapping}).
Aside from symbolic mappings (@pxref{Symbol Mapping}) and the
use of LISTOF constructs,
the generated concrete and abstract syntaxes need to be identical in order
to allow computations to take place during tree construction.

@node Abstraction
@subsection Abstracting information from literals

Literal terminals often distinguish phrases whose structures are identical
except for the particular literal terminal.
For example, in a normal arithmetic expression the phrase describing
addition and the phrase describing subtraction are identical except for
the literal @code{+} or @code{-}.
Taking nonterminal equivalence classes into account, it may be that
@emph{all} phrases representing operations with two operands are identical
except for the operator literal.

When phrases have identical structure except for one or more literals,
the tree computations carried out at the nodes corresponding to those
phrases are often identical except for some parameter that depends on the
particular literal.
It is then useful to abstract from the distinct literals,
@findex literals, abstraction from
@findex abstracting from literals
obtaining a single phrase with which to associate the computation and
a set of phrases with which to associate the parameter evaluation.
The key point here is that in many cases the computation will apply to a
wide variety of translation problems, whereas the particular set of
literals characterizes a single translation problem.
By abstracting from the distinct literals, the computation can be reused.
@findex reuse of computations

To abstract from a specific literal, simply replace that literal with a
nonterminal and add a production that derives the literal from that
nonterminal.
This added production represents the phrase with which the parameter
evaluation would be associated.
The computation for the phrase in which the literal was replaced by
the nonterminal will now obtain the parameter value from the corresponding
child, rather than evaluating it locally.

@node Expression
@subsection Mapping expressions for overload resolution

It is quite common for a single operator to have different meanings that
depend on the types of its operands.
For example, in Pascal the operator @code{+} might mean integer addition,
real addition or set union.
There are well-known techniques for deciding what is meant in a particular
context, and these techniques depend only on the particular set of operators
and operand types.
The computations themselves are parameterized by this information
(@pxref{IdentifyOperator,Selecting an operator at an expression node,
Selecting an operator at an expression node,type,Type Analysis}).

In order to reuse the tree computation to resolve overloading,
@findex overload resolution
@findex resolving overloading
abstract from the particular set of literals
that represent the operators of the language.
Then define equivalence classes in which every nonterminal representing an
expression is replaced by @code{Expr} and every nonterminal representing an
@findex operator
operator by @code{Op}.
Finally, associate the appropriate computations with the following rules:

@example
Expr: Expr Op Expr.
Expr: Op Expr.
Expr: Identifier.
Expr: Integer.
...
@end example

@noindent
(Here @code{...} indicates rules for other denotations, such as
floating-point numbers, @code{true}, etc., defined in the language.)

As an example of the process, consider a language with integer and Boolean
expressions in the style of Pascal.

@menu
* Con::	Description of the input text, abstracting operators
* Sym::	Description of the equivalence classes
@end menu

@ifinfo
@node Con
@subsubsection Description of the input text, abstracting operators
@end ifinfo

The literals that represent operators in this language are @code{+},
@code{-}, @code{*}, @code{/}, @code{div}, @code{mod}, @code{and},
@code{or} and @code{not}.
@findex example of a type-@file{con} file
@findex type-@file{con} file, example
@findex @file{.con} file, example
@findex @file{con} file, example
Define a new nonterminal for each precedence level of the dyadic operators,
one for the unary arithmetic operators, and one for @code{not}:

@example
Addop: '+' / '-' / 'or' .
Mulop: '*' / '/' / 'div' / 'mod' / 'and' .
Sign: '+' / '-' .
Notop: 'not' .
@end example

@noindent
These productions abstract from the literals, and embody the information
about the precedence and association (all operators are left-associative)
needed to determine the phrase structure.

Using these new nonterminals, define the phrase structure of an expression:

@example
SimpleExpression: Sign Sum / Sum .
Sum: Sum Addop Term / Term .
Term: Term Mulop Factor / Factor .
Factor: Notop Factor / Primary .
Primary: Integer / Id / 'true' / 'false' / '(' SimpleExpression ')' .
@end example

@noindent
(Here @code{Integer} is a terminal representing arbitrary digit sequences
and @code{Id} is a terminal representing arbitrary identifiers.
These symbols will be recognized by the lexical analyzer.)

@ifinfo
@node Sym
@subsubsection Description of the equivalence classes
@end ifinfo

All of the dyadic operators fall into the same equivalence class, which
should be represented by the symbol @code{Binop}.
@findex example of a type-@file{map} file
@findex type-@file{map} file, example
@findex @file{.map} file, example
@findex @file{map} file, example
@code{Sign} and @code{Notop} both belong to the @code{Unop} class, and
@code{SimpleExpression}, @code{Sum}, @code{Term}, @code{Factor},
@code{Primary} are in the @code{Expr} class.
Here is a type-@file{map} file defining these classes:

@example
MAPSYM
Op ::= Addop Mulop Sign Notop .
Expr ::= SimpleExpression Sum Term Factor Primary .
@end example

@node Conflicts
@chapter How to Resolve Parsing Conflicts

Eli attempts to construct a particular kind of parser
from the context-free grammar specifying the desired phrase structure.
If this attempt fails, Eli reports that failure by describing a set of
@dfn{conflicts}.
@findex conflict
In order to understand what these conflicts mean, and to understand how
they might be resolved, it is necessary to have a rudimentary idea of how
the constructed parser determines the phrase structure of the input text.

A context-free grammar is said to be @dfn{ambiguous}
@findex ambiguity
if it permits more than one phrase structure to describe a single input text.
Most conflicts are the result of such ambiguities, and
there are three ways of resolving
@findex resolving ambiguity, general methods
them:

@enumerate
@item
Change the grammar so that only one phrase structure is possible.

@item
Provide additional information that causes the parser to select one of the
set of phrase structures.

@item
Change the form of the input text to avoid the ambiguity.
@end enumerate

@noindent
Note that all of these methods result in the parser recognizing a different
language than the one described by the original grammar.

@menu
* Parsing::	How the generated parser determines phrase structure
* Changes::	Conflict resolution by changing the grammar
* Modifiers::	Conflict resolution by ignoring possible structures
@end menu

@node Parsing
@section How the generated parser determines phrase structure

The generated parser
@findex parser operation
is a finite-state machine
@findex finite-state machine
with a stack
@findex stack of parser states
of states.
@findex state of the parser
This machine examines the input text from left to right, one basic symbol
@findex basic symbol, how parser accepts
@findex accepting a basic symbol
at a time.
The current state of the machine is the one at the top of the stack.
It defines the set of productions the parser might be recognizing,
and its progress
@findex progress in recognizing a phrase
in recognizing each.
For example, consider the following trivial grammar:

@example
Sentence: Expression.
Expression: Primary.
Expression: Expression '+' Primary.
Primary: Integer.
Primary: Id.
@end example

Initially, the parser might be recognizing the first production, but in
order to do so it must recognize either the second or the third.
In order to recognize the second production, it must recognize either the
fourth or fifth.
Finally, because we are considering the initial situation, no progress has
been made in recognizing any of these productions.
All of the information expressed by this paragraph is represented by the
initial state, which is the only element of the stack.

On the basis of the state at the top of the stack, and the basic symbol
being examined, the machine decides on one of two moves:
@findex move by the parser

@table @code
@item Shift
@findex shift move by the parser
Accept the basic symbol as the corresponding terminal, push a new
state onto the stack, and examine the next basic symbol.

@item Reduce
@findex reduce move by the parser
Note that a specific phrase has been recognized, remove a number of states
equal to the number of symbols in the sequence of the corresponding
production from the stack, push a new state onto the stack,
and examine the current basic symbol again.
@end table

@noindent
The parser halts
@findex halt of the parser
after the reduce move noting that the production containing the axiom
@findex axiom, recognition of
has been recognized.

If the first basic symbol of the text were an identifier,
a parser for the sample grammar would make a shift move.
The new state would be one in which the parser had completely recognized
the fifth production.
Regardless of the next basic symbol, the parser would then make a reduce
move because the fifth production has been recognized.
One state would be removed from the stack, and a new state pushed
in which the the parser had completely recognized the second production.
Again the parser would make a reduce move, removing one state from the
stack and pushing a state in which the parser had either completely
recognized the first production or recognized the first symbol of the
third production.

The parser's next move is determined by the current input symbol.
@findex move determined by input symbol
@findex parser move determined by input symbol
If the text is empty then the parser makes the reduce move noting that the
first production has been recognized and halts.
If the current symbol of the text is '+' then the parser makes a shift
move.

A conflict occurs when the information available (the current state and the
basic symbol being examined) does not allow the parser to make a unique
decision.
If either a shift or a reduce is possible,
the conflict is a @dfn{shift-reduce conflict};
@findex shift-reduce conflict
if more than one phrase could have been recognized,
the conflict is a @dfn{reduce-reduce conflict}.
@findex reduce-reduce conflict

@menu
* Shift-reduce::	Example of a shift-reduce conflict
* Reduce-reduce::	Example of a reduce-reduce conflict
@end menu

@ifinfo
@node Shift-reduce
@subsection Example of a shift-reduce conflict
@end ifinfo

The classic example of a shift-reduce conflict
@findex example of a shift-reduce conflict
is the so-called ``dangling else problem'':
@findex dangling else problem

@example
Statement: 'if' Expression 'then' Statement.
Statement: 'if' Expression 'then' Statement 'else' Statement.
@end example

A parser built from a grammar containing these productions will have at
least one state in which it could be recognizing either, and has just
completed recognition of the @code{Statement} following @code{then}.
Suppose that the current basic symbol is @code{else};
what move should the parser make next?

Clearly it could shift, accepting the @code{else} and
going to a state in which it is recognizing the second production and
has just completed recognition of the @code{else}.
It could also reduce, however, recognizing an instance of the first
production, popping four elements from the stack and returning to the
current state.
Thus there is a shift-reduce conflict.

The conflict here is due to an ambiguity in the grammar.
Consider the following input text (E1 and E2 are arbitrary expressions, S1
and S2 are statements that do not contain @code{if}):

@example
if E1 then if E2 then S1 else S2
@end example

@noindent
There are two possible phrase structures for this text, depending on
whether the @code{else} is assumed to belong with the first or second
@code{if}:

@example
if E1 then @{if E2 then S1@} else S2
if E1 then @{if E2 then S1 else S2@}
@end example

@noindent
In each case the bracketed sub-sequence is a @code{Statement}
according to one of the given rules, and the entire line is a
@code{Statement} according to the other.
Both are perfectly legal phrase structures according to the grammar.

@ifinfo
@node Reduce-reduce
@subsection Example of a shift-reduce conflict
@end ifinfo

The following description of integer denotations in various bases
leads to a reduce-reduce conflict:
@findex example of a reduce-reduce conflict

@example
Denotation: Seq / Seq Base.
Seq: Digit / Seq Next.
Next: Digit / Hexit.
Digit: '0' / '1' / '2' / '3' / '4' / '5' / '6' / '7' / '8' / '9'.
Hexit: 'a' / 'b' / 'c' / 'd' / 'e' / 'f'.
Base: 'b' / 'o' / 'e' / 'x'.
@end example

@noindent
When @code{Base} is omitted, the integer is assumed to be decimal if it
contains no @code{Hexit} and hexadecimal otherwise.
An explicit @code{Base} indicates the base of the digits to be 2, 8, 10 or 16
respectively.

One of the states of the parser constructed from this grammar indicates
that either the @code{Hexit} @code{b} or the @code{Base} @code{b} has been
recognized.
If the input is not empty then the parser has recognized a @code{Hexit},
but either is possible if the input is empty.
Thus the parser cannot determine the production by which to reduce, and the
conflict arises.

This conflict indicates an ambiguity in the grammar, exemplified by the
input text ``1b''.
Two phrase structures are possible, one yielding the value ``1 base 2'' and
the other yielding the value ``1b base 16''.

@node Changes
@section Conflict resolution by changing the grammar

An ambiguity can sometimes be resolved by changing the grammar.
@findex ambiguity resolved by grammar changes
@findex resolving ambiguity by grammar changes
@findex grammar changes to resolve ambiguity
The altered grammar must define exactly the same set of input texts as the
grammar that gave rise to the conflict, but it cannot describe more than
one phrase structure for any particular text.
That phrase structure must reflect the meaning of the text as defined by
the language design.

Most languages solve the dangling else problem
@findex dangling else solved by grammar change
by associating an @code{else} with the closest @code{if}.
Here is an unambiguous grammar describing that phrase structure:

@example
Statement: matched / unmatched.
matched:
   'if' Expression 'then' matched 'else' matched /
   Others.
unmatched:
   'if' Expression 'then' matched 'else' unmatched /
   'if' Expression 'then' Statement.
@end example

@noindent
(@code{Others} stands for all sequences by which @code{Statement}
could be replaced that contain no @code{if}.)

If the identifiers @code{Statement}, @code{matched} and @code{unmatched}
are placed in an equivalence class, then this grammar yields exactly the
same phrase structure as the ambiguous grammar given in the previous
section.
It is therefore acceptable as far as the remainder of the translation
problem is concerned.

@node Modifiers
@section Conflict resolution by ignoring possible structures

When Eli is constructing a parser from a grammar,
@findex parser construction
it computes a set of symbols called the @dfn{exact right context}
@findex exact right context
for each production in each state.
The exact right context of a production in a state contains all of the symbols
that could follow the phrase associated with that production in that state.
It is possible for the parser to reduce by a production if the current
state indicates that all of the symbols in the production's sequence have
been accepted, and the next basic symbol of the input is a member of the
exact right context of that production in that state.

By adding a @dfn{modification}
@findex modification specification
@findex ambiguity resolved by modifications
@findex resolving ambiguity by modifications
@findex modifications to resolve ambiguity
to the description of a production in a
type-@file{con} file, the user can specify that a particular symbol be
deleted from one or more exact right contexts.
The user is, in effect, telling Eli that these symbols cannot follow the
phrase associated with that production in that state.
In other words, the parser is to ignore phrase structures in which the
specified symbol follows the phrase.

A modification is a sequence consisting of either a dollar (@kbd{$})
@findex $ modification
or at-rate-of (@kbd{@@})
@findex @@ modification
followed by a terminal.
@findex terminal, use in a modification
It can be placed anywhere within a production, and more than one
modification can appear in a single production.
If a modification is introduced but no conflict is resolved thereby,
an error is reported.
@findex error reported in a modification
@findex reported error in a modification

@menu
* Dollar::	The effect of a $-modification
* At::		The effect of a @@-modification
@end menu

@ifinfo
@node Dollar
@subsection The effect of a $-modification
@end ifinfo

Suppose that a modification @code{$S} is introduced into a production
@code{P}.
The effect of this modification is to delete the symbol @code{S} from the
exact right context of production @code{P}.
This kind of modification can be used to solve the dangling else problem:
@findex dangling else solved by $ modification
@findex shift-reduce solved by $ modification 
@findex conflict solved by $ modification

@example
Statement: 'if' Expression 'then' Statement $'else'.
Statement: 'if' Expression 'then' Statement 'else' Statement.
@end example

@noindent
The modification introduced into the first production removes @code{else} from
the exact right context of that production, and therefore makes a reduce
move impossible for the parser when it is in the state indicating that it
is recognizing one of these productions and has just recognized the first
@code{Statement}.
Since the reduce move is impossible, there is no shift-reduce conflict.

@ifinfo
@node At
@subsection The effect of a @@-modification
@end ifinfo

Suppose that a modification @code{@@S} is introduced into a production
@code{P}.
The effect of this modification is to delete the symbol @code{S} from the
exact right context of any production involved in a reduce-reduce conflict
with production @code{P}.
This kind of modification can be used to solve the integer denotation
problem:
@findex reduce-reduce solved by @@ modification
@findex conflict solved by @@ modification

@example
Denotation: Seq / Seq Base.
Seq: Digit / Seq Next.
Next: Digit / Hexit.
Digit: '0' / '1' / '2' / '3' / '4' / '5' / '6' / '7' / '8' / '9'.
Hexit: 'a' / 'b' / 'c' / 'd' / 'e' / 'f'.
Base: 'b' @@EOF / 'o' / 'e' @@EOF / 'x'.
@end example

@noindent
The two modifications introduced into the productions remove @code{EOF}
@findex @code{EOF}
@findex empty input text, representation of
@findex terminal @code{EOF} to terminate text
(the empty input text) from the exact right contexts of the @code{Hexit}
productions that conflict with these two @code{Base} productions, and
therefore make it impossible to reduce the @code{Hexit} productions when
the parser is in the state indicating it has completed recognizing either a
@code{Hexit} or @code{Base} and the input is empty.
A @kbd{b} or @kbd{e} at the end of an input text will thus always be
interpreted as a marker: ``1b'' means ``1 base 2'', not ``1b base 16''.
(``1b base 16'' would have to be written as ``1bx''.)

@node Actions
@chapter Carrying Out Actions During Parsing

In some cases the translation problem being solved requires that arbitrary
actions
@findex actions during parsing
@findex parser actions
@findex arbitrary actions during parsing
@findex user-defined actions during parsing
be carried out as the parser is recognizing the phrase structure of
the input, rather than waiting for the complete phrase structure to be
available.
Most of those cases can be classified either as interactive applications or
as complex structuring problems in which contextual information is needed
to determine the phrase structure.

An arbitrary action is specified by a fragment of C code.
@findex C code
None of the data accessed by this code is provided by Eli; it is the
responsibility of the writer of the arbitrary actions to manage any data
they manipulate.
The simplest approach is to implement all actions as invocations of
operators exported by a library or user-defined abstract data type.
@findex abstract data type
If these invocations have arguments, they are either constant values
characteristic of the particular invocation or references to an entity
exported by some (possibly different) abstract data type.

An action is a sequence consisting of an ampersand (@kbd{&})
@findex & marker for actions
followed by a literal.
The content of the literal is the C code fragment to be executed.
Actions can be placed anywhere in a production, and will be executed when
all of the symbols to the left of the action's position have been
recognized.
Thus an action placed at the end of the production would be executed when
all of the symbols in the sequence have been recognized.

Here is a fragment of a grammar describing a desk calculator;
actions are used to compute subexpression values as the expression is
parsed:
@findex example of user-defined actions

@example
expression:
   term /
   expression '+' term &'ExprPlus();' /
   expression '-' term &'ExprMinus();' .

term:
   primary /
   term '*' primary &'ExprTimes();' /
   term '/' primary &'ExprDiv();' .
@end example

@noindent
The C code fragments invoke operations of a module that maintains a stack
of integer values.

If an action is placed anywhere other than the end of the production,
it may lead to conflicts.
@findex conflict due to user-defined actions
Suppose that an action is placed between the first and second symbols of
the sequence in a production @code{P}.
Suppose further that there is another production, @code{Q}, whose sequence
begins with the same two symbols but does not contain the same action.
If one of the states of the parser could be recognizing either @code{P} or
@code{Q}, and has recognized the first symbol, it would not be able to
decide whether or not to execute the action.

@node Error Recovery
@chapter Improving Error Recovery in the Generated Parser

In some cases, the same pattern in the input text may represent
different tokens in the grammar.  Knowing which token the pattern
represents may be based on other available information.
When the parser determines that it cannot accept the next
look-ahead token, the boolean function @code{Reparatur} is
@findex Reparatur
called:

@example
int Reparatur (POSITION *coord, int *syncode, int *intrinsic);
@end example

This allows the user to change the look-ahead token based on
other available information.  If the function returns @code{0},
then the token has not been altered and the generated parser
continues with its normal error recovery.  If the function returns
@code{1}, it is assumed that the passed in attributes of the token
have been changed (in particular @code{syncode}), and the generated
parser rechecks the look-ahead token to see if it can accept it.

@findex dfltrepar.c
By default, the Eli system provides file @file{dfltrepar.c}
containing a definition of the function @code{Reparatur} that
always returns @code{0}.  To override the default, the user must
provide a new definition of the function @code{Reparatur} in some C file.

In case of erroneous input the generated parser invokes its error
recovery.  The error recovery works completely automatically and
usually behaves satisfactorily, in that it produces a tree that is close to
the one that might be expected if there were no syntactic errors.
This enables the compiler to go on and detect additional semantic errors.

It is also possible to generate a program that will terminate after parsing
if syntactic errors were detected.
To generate a program with this property, simply add the following
parameter to the request for derivation
(@pxref{define,,,pp,Products and Parameters Reference}):

@findex STOPAFTERBADPARSE
@findex syntax errors, stop after detecting
@findex stop after detecting syntax errors
@example
+define='STOPAFTERBADPARSE'
@end example

There are a few possibilities to
control the error recovery in order to improve its behavior.  To
understand the control facilities it is necessary to know how the
error recovery works in principle.

If an error in the input is detected two methods for error repair
are used.  The first method tries to "correct" the error by deleting,
inserting, or replacing one input symbol.  The repair is considered
successful, if the next 4 parsing steps don't lead to another error.
The use of this method is optional.  If the first method is not used
or if it failed the second method performs a complete correction
without backtracking.  It skips input symbols until a so-called
@dfn{restart point} is reached.  The restart point is a symbol
@findex restart point
where normal parsing can be resumed.  Before normal parsing resumes
error correction takes place.  Input symbols are inserted in order
to construct a syntactically correct input and the associated
semantic actions are executed.  The intention is to pass consistent
information to the following compiler phases, which therefore do not
have to bother with syntax errors.

The second method for error recovery can be controlled by providing
additional information.  The intention is to decrease the probability
of error avalanches caused by wrong error repair decisions.  As a
running example, we use an ALGOL-like language defined by the following
grammar:

@example
block : 'begin' declarations statements 'end' .
declarations : declarations declaration ';' / .
declaration : 'real' 'identifier' /
                / 'procedure' 'identifier' ';' statement .
statements : statement / statements ';' statement .
statement : 'identifier' / block .
@end example

Three types of error recovery information can be specified by the
user in files of type @file{.perr}:

@menu
* Separators::           List Separators
* Brackets::             Semantic Brackets
* Skip Symbols::         Unsafe Restart Points
@end menu

@ifinfo
@node Separators
@section List Separators
@end ifinfo

The error recovery has a major drawback when applied to errors
in lists, defined, e.g., as

@example
statements : statement / statements ';' statement .
@end example

A missing delimiter ';' cannot be inserted in order to parse the rest
of the list.  This could lead to an infinite loop in the parser.
Therefore errors like

@example
begin identifier begin identifier ; ...
@end example

cannot be repaired by inserting the semicolon ';' but by deleting
the two symbols 'begin' and 'identifier'.

The following specification in a @file{.perr} file defines the
mentioned terminals as list separators.
@findex list separators

@example
$SEPA ';' . ',' .
@end example

A list separator will always be inserted if a restart point can
be found immediately behind it.  In this case the rest of the
list can be parsed without the danger of getting into an infinite
loop.

@ifinfo
@node Brackets
@section Semantic Brackets
@end ifinfo

Programming languages have bracketed structures like 'begin'
@findex semantic brackets
and 'end' which delimit not only the syntactic structure of
"block" but also the scope of identifiers.  Deleting or inserting
such semantically significant parentheses is highly probably to
cause avalanches of syntactic and semantic errors.  Therefore,
the error recovery should not change the structures of a program
as far as it concerns scopes of identifiers or similar semantic
concepts.

Consider the following erroneous input:

@example
begin
  procedure identifier ;
  begin
    real identifier ;
    identifier ;
    real identifier ;
    identifier ;
@end example

Inserting the terminal 'end' before the second "real declaration"
corrects the program syntactically but may lead to a semantic error
in the last line, as the scope structure is changed.

The specification

@example
$BRACKET 'begin' . 'end' .
@end example

in a file of type @file{.perr} declares the mentioned terminals to
be delimiters of semantically significant regions (semantic delimiters).
@findex semantic delimiters
These terminals are not inserted unless the restart point is end of input
or the restart point itself is specified as such a delimiter.

@ifinfo
@node Skip Symbols
@section Unsafe Restart Points
@end ifinfo

Usually there are a few terminals not suited as restart points.
The reason is that is programming languages terminals like
'identifier' or 'number' occur in many different syntactic
positions.  Consider the error

@example
begin real identifier identifier ; real identifier ...
@end example

There is no safe way to tell whether the second identifier belongs to
a statement or to a declaration.  If it is used as a restart point, the
error is corrected to

@example
begin real identifier ; identifier ; real identifier ...
@end example

This corresponds to a transition from the declaration part into the
statement part of the block, a frequent cause for error avalanches.
In general, terminals like 'identifier' or 'number' are not feasible
as restart points.

The specification

@example
$SKIP 'identifier' . 'integer_number' . 'real_number' .
@end example

in a type @file{.perr} file defines the mentioned terminals as
unsafe restart points.  Unsafe restart points are skipped in case
of an error in order to search for restart points more feasible.

With the above specification the second identifier in the mentioned
example will be skipped.  Parsing resumes at the following semicolon
without carrying out a transition to the statement part.

@node Foreign
@chapter Using Foreign parsers

When Eli is used to generate a parser, Maptool is able to relate the
concrete syntax to the abstract syntax and create all of the code necessary
to build a tree representing the input text.
If a parser is generated by other tools, or written by hand, tree-building
code must be created manually.
In this section, we assume that a parser for the source language exists,
and that Eli is being used to generate code from a LIDO specification of
the abstract syntax and desired tree computations.

The interface specification of any parser designed to support tree
computation defines a set of function invocations that will occur as
parsing of the input text proceeds.
If the parser has been generated, these function invocations are included
in the grammar as semantic actions
(@pxref{Actions,Carrying Out Actions During Parsing,
Carrying Out Actions During Parsing}).

The code generated from a LIDO specification includes a set of tree
construction functions, one for each rule context
(@pxref{Tree Construction Functions,,,lidoref,LIDO - Reference Manual}).
These functions must be invoked at appropriate times with appropriate
arguments during the course of the parse.
In order to use an existing parser, therefore, we must implement a module
obeying the interface specification of that parser and correctly invoking
the tree construction functions generated from the LIDO specification.

It would be possible to develop the module in isolation and then integrate
it with the foreign parser, but a better approach is to use the foreign
parser as part of the Eli specification of the complete program.
Development can then proceed incrementally using Eli tools like execution
monitoring to track down errors and verify correct behavior.

@menu
* Nodes::        Building tree nodes
* Coordinates::  Adding coordinate information
* Lists::        Building LISTOF constructs
* Integration::  Running a foreign parser under Eli
@end menu

@node Nodes
@section Building tree nodes

A typical parser interface specifies a data structure to define text
fragments, in addition to the semantic actions:

@example
typedef struct @{  /* Basic symbol */
  int line;       /*   Source line containing the symbol */
  int col;        /*   Column containing the first character */
  int type;       /*   Classification code of the symbol */
  char *text;     /*   Symbol text */
@} Token;
                  /* Symbol classification codes */
#define ETXT 1    /*   End of the shource file */
#define LPAR 2    /*   Left parenthesis */
#define RPAR 3    /*   Right parenthesis */
#define PLUS 4    /*   Plus */
#define STAR 5    /*   Asterisk */
#define INTG 6    /*   Integer */
...
@end example

Each tree construction function generated by Eli from a LIDO specification
must be invoked with pointers to its children, and therefore the tree must
be built bottom-up.
The usual strategy is to store pointers to constructed nodes on a stack
until their parent node is built.
Eli provides a stack module for this purpose:

@example
NODEPTR *_nst;         /* Stack array: _nst[...] are the elements */
int _nsp;              /* Stack index: _nst[_nsp] is the top element */
void _incrnodestack(); /* Push an empty element onto the stack */
@end example
@noindent
Elements of the stack are @code{_nst[_nsp]}, @code{_nst[_nsp-1]}, etc.
The statement @code{_nsp-=k;} pops @code{k} elements off of the stack,
and the statement @code{_incrnodestack();} pushes an empty element onto the
stack.
To make the stack visible, include the file @file{treestack.h}.

The behavior of the functions called by the parser is determined primarily
by the needs of the abstract syntax.
We'll consider two LIDO specifications, one for computing the value of an
integer expression involving addition and multiplication and the other for
carrying out overload resolution in more general expressions.

@menu
* Evaluate::    Tree designed for expression evaluation
* Overload::    Tree designed for overload resolution
* Chain nodes:: Tree nodes for chain rules
@end menu

@node Evaluate
@subsection Tree designed for expression evaluation

Consider the following LIDO specification, which evaluates an integer
expression involving addition and multiplication.
It assumes each @code{Integer} terminal is represented by the value of the
corresponding integer:

@example
ATTR val: int;

RULE Top: Root ::= Expr COMPUTE
  printf("The value is %d\n", Expr.val);
END;

RULE Add: Expr ::= Expr '+' Expr COMPUTE
  Expr[1].val=ADD(Expr[2].val,Expr[3].val);
END;

RULE Mul: Expr ::= Expr '*' Expr COMPUTE
  Expr[1].val=MUL(Expr[2].val,Expr[3].val);
END;

RULE Num: Expr ::= Integer COMPUTE
  Expr.val=Integer;
END;
@end example
@noindent
Eli generates four node construction functions from this specification:

@example
NODEPTR MkTop(POSITION *_coord, NODEPTR _d1);
NODEPTR MkAdd(POSITION *_coord, NODEPTR _d1, NODEPTR _d2);
NODEPTR MkMul(POSITION *_coord, NODEPTR _d1, NODEPTR _d2);
NODEPTR MkNum(POSITION *_coord, int _TERM1);
@end example
@noindent
To make the node construction functions visible, include the file
@file{treecon.h}.

The @code{_coord} parameter will be discussed in detail in the next
section; here we will always supply @code{NoPosition} as the value of this
argument
(@pxref{error,Source Text Coordinates and Error Reporting,
Source Text Coordinates and Error Reporting,lib,The Eli Library}).

Our module must call @code{MkNum} whenever the parser recognizes an
integer, and we must provide the internal value of that integer as
the second argument of that call.
The result of the call must be pushed onto the top of the stack.

Suppose that by looking at the code of the parser, or the grammar from
which the parser was generated, we determine that when the parser
recognizes an integer in the input text it calls the function
@code{int_literal_constant} with the @code{Token} describing that integer
as its argument.
We might then implement @code{int_literal_constant} as follows:

@example
void int_literal_constant(Token *t)
@{ _incrnodestack();
  _nst[_nsp]=MkNum(NoPosition,atoi(t->text));
@}
@end example
@noindent
Note that this code does @emph{not} check for an error in the conversion of
the string.
That might or might not be reasonable, depending upon how careful the
parser was in accepting a string as a representation of an integer value.

Further examination of the parser might show that it calls the function
@code{mult_operand} with no arguments when it has recognized an expression
involving two operands and an asterisk operator.
In this case, the nodes for the two operand expressions are already on the
stack.
They must be removed and replaced by a @code{Mul} node:

@example
void mult_operand(void)
@{ _nst[_nsp-1]=MkMul(NoPosition,_nst[_nsp-1],_nst[_nsp]);
  _nsp--;
@}
@end example
@noindent
Implementation of the action when the parser recognizes an expression
involving two operands and a plus operator is identical except that
@code{MkAdd} is invoked instead of @code{MkMul}.

If the parser invokes @code{level_0_expr} with no arguments when it has
completed recognition of the input text, the implementation of that
function might be:

@example
void level_0_expr(void)
@{ _nst[_nsp]=MkTop(NoPosition,_nst[_nsp]);
@}
@end example

Suppose that the parser invokes @code{level_3_expr} with no arguments when
it has recognized an expression in parentheses.
There is no corresponding rule in the abstract syntax, because parentheses
serve only to override operator precedence and do not affect the
computation.
In that case, the routine does nothing:

@example
void level_3_expr(void)
@{ @}
@end example

An expression language usually has precedence levels containing several
operators.
For example, dyadic @code{+} and @code{-} operators usually have the same
precedence, as do dyadic @code{*} and @code{/}.
A parser may invoke a single function when it recognizes @emph{any}
dyadic expression whose operator is at a specific precedence level.
In that case, some indication of the operator must be passed to that
function.
For example, the parser might call @code{mult_operand} with a pointer to
the operator token.
The implementation of @code{mult_operand} must then use the token type to
select the correct node construction function:

@example
void mult_operand(Token *o)
@{ if (o->type == STAR)
    _nst[_nsp-1]=MkMul(NoPosition,_nst[_nsp-1],_nst[_nsp]);
  else
    _nst[_nsp-1]=MkDiv(NoPosition,_nst[_nsp-1],_nst[_nsp]);
  _nsp--;
@}
@end example
@noindent
This assumes that the parser will not invoke @code{mult_operand} unless the
operator is either @code{*} or @code{/}, and therefore no error checking is
required.
(If the number of operators at the precedence level were larger, then a
switch statement might be preferable to the conditional.)

The code for @code{mult_operand} also assumes that the division is
implemented by a LIDO rule named @code{Div}:

@example
RULE Div: Expr ::= Expr '/' Expr COMPUTE
  Expr[1].val=DIV(Expr[2].val,Expr[3].val);
END;
@end example

@node Overload
@subsection Tree designed for overload resolution

Consider the following LIDO specification, which provides a structure to
analyze the result type of an expression involving addition, subtraction,
multiplication, and division of integers and floating point numbers
(@pxref{Operator,Operator Overloading,Operator Overloading,typetutorial,
Tutorial on Type Analysis}).
It assumes that each @code{Integer} and @code{Float} terminal
is represented by the string definining the corresponding number:

@example
RULE Top: Root ::= Expr END;

RULE Dya: Expr ::= Expr BinOp Expr END;

RULE Pls: BinOp ::= '+' END;
RULE Min: BinOp ::= '-' END;
RULE Str: BinOp ::= '*' END;
RULE Sls: BinOp ::= '/' END;

RULE Ntg: Expr ::= Integer END;
RULE Flt: Expr ::= Float   END;
@end example
@noindent
Eli generates eight node construction functions from this specification.
The first six are:

@example
NODEPTR MkTop(POSITION *_coord, NODEPTR _d1);
NODEPTR MkDya(POSITION *_coord, NODEPTR _d1, NODEPTR _d2, NODEPTR _d3);
NODEPTR MkPls(POSITION *_coord);
NODEPTR MkMin(POSITION *_coord);
NODEPTR MkAst(POSITION *_coord);
NODEPTR MkSls(POSITION *_coord);
@end example
@noindent
To make the node construction functions visible, include the file
@file{treenode.h}.

In this example, we would like to represent the integer and floating-point
constants in the tree by the strings that represent them in the source text.
There are two possibilities:

@enumerate
@item
If the foreign parser stores permanent copies of token strings, then
pointers to those strings can be stored in the tree nodes.

@item
If the foreign parser points to token strings in the input buffer, then
our module must store them permanently for reference by the tree nodes.
@end enumerate

In case 1, a specification must be added to the LIDO description
of the tree:

@example
TERM Integer, Float: CharPtr;
@end example
@noindent
@code{CharPtr} is the LIDO name for the C type @code{char *}.
The definition of @code{CharPtr} is made available by including file
@file{strings.h}.

The @code{TERM} specification causes the two functions @code{MxNtg} and
@code{MkFlt} to be defined as follows:

@example
NODEPTR MkNtg(POSITION *_coord, CharPtr t);
NODEPTR MkFlt(POSITION *_coord, CharPtr t);
@end example

Suppose that, as discussed in the last subsection, the parser calls
@code{int_literal_constant} when it recognizes an integer in the source
text.
That routine could be implemented as:

@example
void int_literal_constant(Token *t)
@{ _incrnodestack();
  _nst[_nsp]=MkNtg(NoPosition,t->text);
@}
@end example

In case 2, we can make use of Eli's @code{MakeName} module
(@pxref{MakeName,Generating Optional Identifiers,
Generating Optional Identifiers,problems,Solutions of common problems}).
It provides a function to store a string uniquely and return an
integer-valued hash table index to that unique representation:

@example
int MakeName(char *c);
@end example
@noindent
Because the default type of a LIDO terminal is @code{int}, we can omit the
@code{TERM} specification and the two functions @code{MxNtg} and
@code{MkFlt} will be defined as follows:

@example
NODEPTR MkNtg(POSITION *_coord, int t);
NODEPTR MkFlt(POSITION *_coord, int t);
@end example
@noindent
The implementation of @code{int_literal_constant} would be:

@example
void int_literal_constant(Token *t)
@{ _incrnodestack();
  _nst[_nsp]=MkNum(NoPosition,MakeName(t->text));
@}
@end example

The @code{MakeName} module must be instantiated
in order to gain access to the @code{MakeName} function.
This is done by adding the following line to a @file{.specs} file:

@example
$/Tech/MakeName.gnrc:inst
@end example
@noindent
No @code{+instance} parameter should be supplied, because scanning and
parsing are provided by the foreign code.
Once the module has been instantiated, the definition of the @code{MakeName}
function is made available to the tree construction module by including file
@file{MakeName.h}.

Let's assume that the parser invokes a single function when it recognizes
any dyadic expression whose operator is at a specific precedence level,
passing the operator token to that function.
For example, @code{+} and @code{-} might both be operators at precedence
level 1:

@example
void level_1_operator(Token *o)
@{ NODEPTR op;
  if (o->type == PLUS) op=MkPls(NoPosition);
  else                 op=MkMin(NoPosition);
  _nst[_nsp-1]=MkDya(NoPosition,_nst[_nsp-1],op,_nst[_nsp]);
  _nsp--;
@}
@end example
@noindent
This assumes that the parser will not invoke @code{level_1_operator} unless
the operator is either @code{+} or @code{-}, and therefore no error checking
is required.
(If the number of operators at the precedence level were larger, then a
switch statement might be preferable to the conditional.)

If, on the other hand, the parser invokes a function @code{add_operand}
with no arguments when it has recognized an expression involving two operands
and an addition operator then @code{add_operand} can be implemented as:

@example
void add_operand(void)
@{ _nst[_nsp-1]=
    MkDya(NoPosition,_nst[_nsp-1],MkPls(NoPosition),_nst[_nsp]);
  _nsp--;
@}
@end example
@noindent
Note that the operator node implied by the @code{add_operand} call must be
explicitly created in this case; it is only implicit in the parse.

@node Chain nodes
@subsection Tree nodes for chain rules

Recall that a chain rule has the form @samp{X ::= Y},
where @samp{X} differs from @samp{Y}
(@pxref{Chain Rules,,Chain rule definitions}).
Such a rule will always result in a tree node with a single child, and if
the rule name is @samp{Ch} then the constructor function will be:

@example
NODEPTR MkCh(POSITION *_coord, NODEPTR _d1);
@end example

With the exception of the root node of the tree, it is never necessary to
explicitly invoke the constructor of a chain rule node.
This is actually a very important property of the tree construction module.
For example, consider the following fragment of a LIDO specification:

@example
RULE SimpleVar: Var ::= VrblIdUse                      END;
RULE SubscrVar: Var ::= Var '[' Exp ']'                END;
RULE VarExp:    Exp ::= Var                            END;
RULE ArrayExp:  Exp ::= TypeIdUse '[' Exp ']' 'of' Exp END;
RULE Typ: TypeIdUse ::= Symbol                         END;
RULE Var: VrblIdUse ::= Symbol                         END;
RULE Idn:    Symbol ::= Identifier                     END;
@end example
@noindent
@code{Identifier} is a terminal symbol, represented by a unique permanent
string
(@pxref{Overload,Tree designed for overload resolution,
Tree designed for overload resolution}).

The problem here is that, given the input sequence @samp{a[}, a parser
would have to look beyond the matching @samp{]} in order to decide whether
@samp{a} was a @code{VrblIdUse} or a @code{TypeIdUse}.
But because the rules @code{Typ} and @code{Var} are chain rules, their
constructor functions don't need to be called.
That means the parser can construct an @code{Idn} node for @samp{a} and
leave it on the stack.
If that node is later used as the left child of a @code{SubscrVar} node,
the tree construction module will insert the necessary @code{Var} and
@code{SimpleVar} nodes.
If, on the other hand, the @code{Idn} node is used as the left child of an
@code{ArrayExp} node then the tree construction module will insert the
necessary @code{Typ} node.
There is no need for the parser to look ahead.

@node Coordinates
@section Adding coordinate information

LIDO computations may access the coordinates of the first character
of the source text region represented by a node.
Usually, these computations are used to attach error reports to appropriate
text locations.
Many of the modules that implement common computations use this facility
for error reporting (for an example,
@pxref{ChkTyped,Verifying typed identifier usage,
Verifying typed identifier usage,type,Type Analysis}).

Execution monitoring is provided by Noosa, a separate process that can
display the abstract syntax tree and
graphically relate it to the source text
(@pxref{Trees,Trees and Attribute Values,Trees and Attribute Values,mon,
Execution Monitoring Reference}).
Noosa requires that both the source text coordinates of the first
character of a tree context and those of the first character
@emph{beyond} that context be supplied to its construction function.

Specific source text coordinates are represented by a @code{POSITION}
(@pxref{error,Source Text Coordinates and Error Reporting,
Source Text Coordinates and Error Reporting,lib,The Eli Library}).
This data type and the operations upon it are made visible by including the
file @file{err.h}.
An appropriate @code{POSITION} value must be created from parser data and a
pointer to that data passed to the tree construction function.

@menu
* Start::  Supplying coordinates for computation
* Span::   Supplying coordinates for Noosa
@end menu

@node Start
@subsection Supplying coordinates for computation

LIDO provides three names that can be used in computations to obtain source
text coordinates of a tree context
(@pxref{Predefined Entities,,Predefined Entities,lidoref,LIDO - Reference Manual}):

@table @code
@item LINE
the source line number of the tree context.
@findex @code{LINE}
@item COL
the source column number of the tree context.
@findex @code{COL}
@item COORDREF
the address of the source coordinates of the tree context,
to be used for example in calls of the message routine of
the error module or in calls of tree construction functions.
@findex @code{COORDREF}
@end table
@noindent
If any of these three names appear in the LIDO computation, the source text
coordinates of the first character of each tree context must be supplied to
its node construction function.
That information must be extracted from the parser.

In order to support the use of coordinates in computation,
the tree construction function must have access to
the location of the first character of its tree context.
We have assumed that each token provided by the parser specifies the line
and column of the first character of the corresponding input string
(@pxref{Nodes,Building tree nodes,Building tree nodes}).
This information can be used to build a @code{POSITION} value:

@example
POSITION curpos;

void int_literal_constant(Token *t)
@{ LineOf(curpos) = t->line; ColOf(curpos) = t->col;
  _incrnodestack();
  _nst[_nsp]=MkNum(&curpos,atoi(t->text));
@}
@end example
@noindent
Notice that the address of @code{curpos}, rather then @code{curpos} itself,
is passed to the node construction function @code{MkNum}.

Unfortunately, this information isn't sufficient.
We must not only pass the coordinates to @code{MkNum}, we must also save
them on the stack in case this node is the left child of another node.
At that point, the coordinates of the first character of this token would
be the coordinates of the first character of the larger tree context.

The Eli stack module actually provides two parallel stacks, @code{_nst} for
nodes and @code{_pst} for positions.
Thus the complete code for integer literal constants would be:

@example
void int_literal_constant(Token *t)
@{ LineOf(curpos) = t->line; ColOf(curpos) = t->col;
  _incrnodestack();
  _pst[_nsp]=curpos;
  _nst[_nsp]=MkNum(&curpos,atoi(t->text));
@}
@end example
@noindent
The position value, not a pointer to that value, is saved on the stack.
That frees @code{curpos} to be used in constructing other values.

When a node whose children are all on the stack is constructed, the
coordinates are obtained from the leftmost child:

@example
void mult_operand(void)
@{ _nst[_nsp-1]=MkMul(&_pst[_nsp-1],_nst[_nsp-1],_nst[_nsp]);
  _nsp--;
@}
@end example

Generally speaking, the stack location for the left operand
becomes the stack location for the result.
Because the coordinates of the result are the coordinates of the left
operand, there is no need for an assignment to @code{_pst}.

@node Span
@subsection Supplying coordinates for Noosa

Noosa requires the coordinates of the first character of a tree context and
also the coordinates of the first character beyond the end of that context.
The additional coordinates should be supplied, however, @emph{only} if
execution monitoring has actually been specified for the particular run.
This is because the @code{POSITION} value will only have the necessary
space if monitoring has been specified.

The simplest strategy is to define a routine to compute the appropriate
@code{POSITION} value for a given token:

@example
POSITION PositionOf(Token_t *token)
@{ POSITION curpos;

  LineOf(curpos) = token->line; ColOf(curpos) = token->col;
#ifdef RIGHTCOORD
  RLineOf(curpos) = LineOf(curpos);
  RColOf(curpos) = ColOf(curpos) + strlen(token->text);
#ifdef MONITOR
  CumColOf(curpos) = ColOf(curpos); RCumColOf(curpos) = RColOf(curpos);
#endif
#endif
  return curpos;
@}
@end example
@code{RIGHTCOORD} and @code{MONITOR} are defined by Eli for each C
compilation if the user specifies the @code{+monitor} parameter to the
derivation
(@pxref{monitor,,,pp,Products and Parameters Reference}).

A node for an integer literal constant would then be built by:

@example
void int_literal_constant(Token *t)
@{ curpos = PositionOf(t);
  _incrnodestack();
  _pst[_nsp]=curpos;
  _nst[_nsp]=MkNum(&curpos,atoi(t->text));
@}
@end example

The construction of a node whose children are on the stack becomes more
complex, because the coordinates of the constructed node involve the
coordinates of the first character of the leftmost child node and the
coordinates of the first character beyond the end of rightmost child node.
The tree stack module provides a function, @code{SpanOf}, to compute the
correct coordintes:

@example
POSITION SpanOf(POSITION left, POSITION right);
@end example

Using @code{SpanOf}, the @code{mult_operand} routine would be written as:

@example
void mult_operand(void)
@{ curpos=SpanOf(_pst[_nsp-1],_pst[_nsp]);
   _pst[_nsp-1]=curpos;
   _nst[_nsp-1]=MkMul(&curpos,_nst[_nsp-1],_nst[_nsp]);
  _nsp--;
@}
@end example

@node Lists
@section Building LISTOF constructs

There are @emph{three} tree construction functions associated with a
LISTOF construct with the rule name @samp{Ex}
(@pxref{Tree Construction Functions,,,lidoref,LIDO - Reference Manual}):

@example
NODEPTR MkEx(POSITION *_coord, NODEPTR _d1);
NODEPTR Mk0Ex(POSITION *_coord);
NODEPTR Mk2Ex(POSITION *_coord, NODEPTR _d1, NODEPTR _d2);
@end example
@noindent
Arguments @samp{_d1} and @samp{_d2} may be:

@itemize @bullet
@item
the result of @code{Mk0Ex}, which represents an empty portion of the list
(any call to @code{Mk0Ex} can be replaced by the constant
@code{NULLNODEPTR})

@item
the result of @code{Mk2Ex}, which represents a portion (possibly empty) of
the list

@item
any node that can be made a list element subtree by implicit insertion of
chain contexts, which represents a single element of the list
@end itemize
@noindent
The node representing the complete @samp{Ex} construct is the one resulting
from a call of @code{MkEx}.

LISTOF constructs always involve either looping or recursion in a parser.
For example, consider a language in which a block consists of an
arbitrary non-empty sequence of declarations and statements.
The LIDO specification for the abstract syntax might contain the rule:

@example
RULE Blk: Block LISTOF Declaration | Statement END;
@end example

Suppose that the parser calls @code{declaration_action} after each
@code{Declaration} has been recognized and @code{statement_action}
after each @code{Statement} has been recognized.
Moreover, it calls @code{block_begin} prior to beginning analysis of the
list and @code{block_end} when the end of the block has been reached:

@example
void block_begin(void)
@{ _incrnodestack();
  _nst[_nsp]=Mk0Blk(&curpos);
@}

void declaration_action(void)
@{ curpos=SpanOf(_pst[_nsp-1],_pst[_nsp]);
  _pst[_nsp-1]=curpos;
  _nst[_nsp-1]=Mk2Blk(&curpos,_nst[_nsp-1],_nst[_nsp]);
  _nsp--;
@}

void statement_action(void)
@{ declaration_action(void) @}

void block_end(void)
@{ _nst[_nsp]=MkBlk(&_pst[_nsp],_nst[_nsp]); @}
@end example

@node Integration
@section Running a foreign parser under Eli

There are two distinct possibilities for the implementation of a foreign
parser:

@itemize @bullet
@item
The foreign parser exists as a collection of C/C++ source files and/or
object files that can be linked with the tree construction and computation
modules.
(A scanner/parser created by LEX/YACC or FLEX/Bison would have this
property.)

@item
The foreign parser exists as an executable file that expects to load a
shared library containing the tree construction and computation modules.
(A scanner/parser created by a Java-based tool like ANTLR would have this
property.)
@end itemize

@menu
* Linked::  The parser is a collection of routines
* Shared::  The parser is an executable file
@end menu

@node Linked
@subsection The parser is a collection of routines

When the parser is a collection of routines, whether in source or object
form, the files containing those routines can be listed in a @file{.specs}
file
(@pxref{Specifications,Descriptive Mechanisms Known to Eli,
Descriptive Mechanisms Known to Eli,novice,Guide for New Eli Users}).
The name of that file then appears in the overall specification.
For example, suppose that all of the components of the foreign parser are
listed in file @file{parser.specs} and the tree computations are defined
by the file @file{treecomp.lido}.
Then the overall specification of the program might be @file{prog.specs}
with the content:

@example
parser.specs
treecomp.lido
@end example
@noindent
(Normally the tree computation would involve a number of different
specifications rather than a single @file{.lido} file, so a more realistic
example would use @samp{treecomp.specs} or @samp{treecomp.fw} to specify it.)

Eli normally generates a parser from every specification.
When a parser is supplied, this behavior must be suppressed by adding the
parameter @code{+parser=none} to the derivation
(@pxref{Interactive,How to Request Product Manufacture,
How to Request Product Manufacture,novice,Guide for New Eli Users}):

@example
prog.specs +parser=none :exe
@end example

Eli also normally provides the following main program:

@example
int main(int argc , char *argv[])
@{
#ifdef MONITOR
  _dap_init (argv[0]);
  _dapto_enter ("driver");
#endif

  ParseCommandLine(argc, argv);

#include "INIT.h"

  TREEBUILD();

#ifdef STOPAFTERBADPARSE
  if (ErrorCount[ERROR] == 0)
#endif
  ATTREVAL();

#include "FINL.h"

#ifdef MONITOR
  _dapto_leave ("driver");
#endif
  return (ErrorCount[ERROR] > 0);
@}
@end example
@noindent
One possible strategy is to write a wrapper procedure named @code{TREEBUILD}
that carries out all of the setup operations needed for the foreign parser
and then invokes it.
This can often be done by renaming a main program provided with the foreign
parser and making a few changes to it.

If it is not feasible to modify the main program of the foreign parser,
then production of Eli's main program must be suppressed by adding the
parameter @code{+nomain} to the derivation:

@example
prog.specs +nomain +parser=none :exe
@end example
@noindent
In this case, however, the interface module must:

@enumerate
@item
include the initialization code file @file{INIT.h},

@item
invoke @code{ATTREVAL} after the tree has been built,

@item
and include the finalization code file @file{FINL.h}.
@end enumerate

If the parser executes a function call @samp{begin_parse();}
before invoking any other functions of the interface,
and a function call @samp{end_parse();} when it has completed recognition
of the input text, then the implementation of these two functions might be:

@example
void begin_parse(void)
@{
#ifdef MONITOR
  _dap_init ("");  /* Argument is generally the program name */
#endif

#include "INIT.h"
@}

void end_parse(void)
@{  _nst[_nsp]=MkRoot(&_pst[_nsp],_nst[_nsp]);
  ATTREVAL();

#include "FINL.h"
@}
@end example
@noindent
Replace "Root" by the name of the rule that creates the root node of the
tree.
If the root node is created by another function, omit the
@code{Mk}-function call.
@code{ATTREVAL} assumes that the root node is at the top of the stack;
if this pre-condition is not satisfied then the computation will silently
do nothing.

@node Shared
@subsection The parser is an executable file

When the parser is an executable file that expects to load a shared
library, that library must be built from the specifications of the tree
construction and computation
(@pxref{so,,,pp,Products and Parameters Reference}).
The library must not contain a parser or a main program:

@example
treecomp.specs +nomain +parser=none :so
@end example
@noindent
Here we assume that all of the components of the LIDO specification,
tree construction interface, and supporting modules are listed in
@samp{treecomp.specs}.

The simplest approach to integrating the foreign parser with the shared
library is to copy it to a file with the name that the foreign parser
expects.
For example, if the parser program expects to load a shared library named
@samp{ParserActions.so}, then use the following derivation to make the
library available under that name:

@example
treecomp.specs +nomain +parser=none :so > libParserActions.so
@end example
@noindent
(See your system documentation for the placement and naming of shared
library files.)

@node Grammars
@appendix Grammars for the Specification Files

@findex type-@file{con} file format
@findex @file{.con} file format
@findex @file{con} file format
@findex file formats
@display
@var{TypeConFile} ::= @var{Production}*.

@var{Production} ::= @var{Identifier} @var{Delim} @var{Alternatives} @samp{.}.

@var{Delim} ::= @samp{:} / @samp{::=}.

@var{Alternatives} ::=
   @var{Alternatives} @samp{/} @var{Alternative} /
   @var{Alternatives} @samp{//} @var{Separator} /
   @var{Alternative} .

@var{Alternative} ::= @var{Element}*.
@var{Separator} ::= @var{Symbol}.

@var{Element} ::=
   @var{Symbol} /
   @var{Connection} /
   @var{Modification} /
   @samp{(} @var{Alternatives} @samp{)} /
   @samp{[} @var{Alternatives} @samp{]} /
   @var{Element} @samp{*} /
   @var{Element} @samp{+} .

@var{Connection} ::= @samp{&} @var{Symbol}.
@var{Modification} ::= @samp{@@} @var{Symbol} / @samp{$} @var{Symbol}.

@var{Symbol} ::= @var{Identifier} / @var{Literal}.




@findex type-@file{map} file, format
@findex @file{.map} file, format
@findex @file{map} file, format
@var{TypeMapFile} ::=
  (@samp{MAPSYM} @var{SymbolMapping}+ / @samp{MAPRULE} @var{RuleMapping}+ / @samp{MAPCHAINS})+ .

@var{SymbolMapping}: @var{Identifier} @samp{::=} @var{Members} @samp{.} .
@var{Members}: @var{Identifier}+ .

@var{RuleMapping}: @var{Rule} @var{Rewrite} @var{RuleName} @samp{.} .
@var{Rule}: @var{Identifier} @var{Delimiter} @var{RHS} .
@var{Delimiter}: @samp{:} / @samp{::=} .
@var{RHS}: @var{Element}* .
@var{Element}: @var{Identifier} / @var{Text} .
@var{Rewrite}: @samp{<} @var{RewriteRHS} @samp{>} .
@var{RewriteRHS}: (@var{Position} / @var{Text})+ .
@var{Position}: @samp{$} @var{Integer} .
@var{RuleName}: / @samp{:} @var{Identifier} .




@findex type-@file{perr} file, format
@findex @file{.perr} file, format
@findex @file{perr} file, format
@var{TypePerrFile} ::= @var{ErrorSpecs}.

@var{ErrorSpecs} ::= @var{ErrorSpecs} @var{SeparatorSpecs} /
                     @var{ErrorSpecs} @var{BracketSpecs} /
                     @var{ErrorSpecs} @var{SkipSpecs} / .

@var{SeparatorSpecs} ::= @samp{$SEPA} @var{Symbols} .

@var{BracketSpecs} ::= @samp{$BRACKET} @var{Symbols} .

@var{SkipSpecs} ::= @samp{$SKIP} @var{Symbols} .

@var{Symbols} ::= @var{Symbols} @var{Symbol} .

@var{Symbol} ::= @var{Identifier} / @var{Literal} .
@end display

@node Index
@unnumbered Index
@printindex fn

@contents
@bye
